{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels, kernel_size=3, *args, **kwargs):\n",
    "    return nn.Conv2d(in_channels, out_channels, \n",
    "                     bias=False, padding=kernel_size//2, kernel_size=kernel_size, *args, **kwargs)\n",
    "\n",
    "def conv_bn(in_channels, out_channels, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        conv(in_channels, out_channels, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_channels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation=nn.ReLU(inplace=True),\n",
    "                 expansion=1, downsampling=1):\n",
    "        super(ResnetResidualBlock, self).__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.blocks = nn.Identity()\n",
    "        self.expansion = expansion\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.downsampling = downsampling\n",
    "        if self.apply_shortcut:\n",
    "            self.shortcut = conv_bn(in_channels, self.expanded_channels,\n",
    "                               stride=self.downsampling, kernel_size=1)\n",
    "    \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.expansion * self.out_channels\n",
    "    \n",
    "    @property\n",
    "    def apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.apply_shortcut: \n",
    "            residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(ResnetResidualBlock):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            # need downsampling here if we are doubling the #filters\n",
    "            conv_bn(in_channels, out_channels, kernel_size=3, stride=self.downsampling),\n",
    "            nn.ReLU(),\n",
    "            conv_bn(out_channels, out_channels, kernel_size=3)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(ResnetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            # need downsampling here if we are doubling the #filters\n",
    "            conv_bn(in_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            conv_bn(out_channels, out_channels, kernel_size=3, stride=self.downsampling),\n",
    "            nn.ReLU(),\n",
    "            conv_bn(out_channels, self.expanded_channels, kernel_size=1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1000,\n",
    "                block_sizes=[64, 128, 256, 512], depths=[2, 2, 2, 2],\n",
    "                block=BasicBlock, activation=nn.ReLU(inplace=True), *args, **kwargs):\n",
    "        super(ResNet, self).__init__()\n",
    "        # input gate\n",
    "        self.gate = nn.Sequential(\n",
    "            conv_bn(in_channels, block_sizes[0], kernel_size=7, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.block_type = block\n",
    "        # middle blocks\n",
    "        self.blocks_list = self._make_layers(block_sizes, activation, depths)\n",
    "        # end decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.blocks_list[-1][-1].expanded_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def _make_layers(self, block_sizes, activation, depths):\n",
    "        in_out_block_sizes = list(zip(block_sizes, block_sizes[1:]))\n",
    "        return nn.ModuleList([\n",
    "            self._make_layer(block_sizes[0], block_sizes[0], activation, n=depths[0]),\n",
    "            *[self._make_layer(in_channels * self.block_type.expansion, \n",
    "                               out_channels, activation=activation, n=n) \n",
    "                  for (in_channels, out_channels), n in zip(in_out_block_sizes, depths[1:])]\n",
    "        ])\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, activation, n=1):\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        return nn.Sequential(\n",
    "            self.block_type(in_channels, out_channels, downsampling=downsampling,\n",
    "                            activation=activation),\n",
    "            *[self.block_type(out_channels * self.block_type.expansion, out_channels,\n",
    "                downsampling=1, activation=activation) for _ in range(n - 1)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks_list:\n",
    "            x = block(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5          [-1, 256, 56, 56]          16,384\n",
      "       BatchNorm2d-6          [-1, 256, 56, 56]             512\n",
      "            Conv2d-7           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-8           [-1, 64, 56, 56]             128\n",
      "              ReLU-9           [-1, 64, 56, 56]               0\n",
      "           Conv2d-10           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 56, 56]             128\n",
      "             ReLU-12           [-1, 64, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-38          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-39          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-40          [-1, 128, 56, 56]             256\n",
      "             ReLU-41          [-1, 128, 56, 56]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "             ReLU-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-80         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-81          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-82          [-1, 256, 28, 28]             512\n",
      "             ReLU-83          [-1, 256, 28, 28]               0\n",
      "           Conv2d-84          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 14, 14]             512\n",
      "             ReLU-86          [-1, 256, 14, 14]               0\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "  BottleneckBlock-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-142           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-143          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-144          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-145          [-1, 512, 14, 14]               0\n",
      "          Conv2d-146            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-147            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-148            [-1, 512, 7, 7]               0\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "         Flatten-174                 [-1, 2048]               0\n",
      "          Linear-175                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.57\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "model = ResNet(3, 1000, depths=[3, 4, 6, 3], block=BottleneckBlock)\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
